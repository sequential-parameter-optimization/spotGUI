{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from spotPython.data.csvdataset import CSVDataset\n",
    "from spotPython.data.pkldataset import PKLDataset\n",
    "from spotPython.utils.init import fun_control_init\n",
    "from spotPython.hyperparameters.values import set_control_key_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun_control = fun_control_init()\n",
    "data_set = \"data.csv\"\n",
    "dataset = CSVDataset(directory=\"./userData/\",\n",
    "                    filename=data_set,\n",
    "                    target_column=\"prognosis\",\n",
    "                    feature_type= getattr(torch, \"int\"),\n",
    "                    target_type=torch.bool)\n",
    "set_control_key_value(control_dict=fun_control,\n",
    "                    key=\"data_set\",\n",
    "                    value=dataset,\n",
    "                    replace=True)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor with data type torch.int\n",
    "tensor = torch.tensor([1, 2, 3], dtype=torch.int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "\n",
    "# Load the ARFF data\n",
    "data, meta = arff.loadarff('./userData/diabetes.arff')\n",
    "\n",
    "# Convert the data to a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "df.to_csv('./userData/diabetes.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'class': 'target'})\n",
    "# Save the DataFrame as a PKL file (pickle):\n",
    "df.to_pickle(\"./userData/diabetes.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame\n",
    "\n",
    "Assume that I have \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 123\n",
      "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/utilities/parsing.py:198: Attribute 'act_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['act_fn'])`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| name            | type   | default   |   lower |   upper | transform             |\n",
      "|-----------------|--------|-----------|---------|---------|-----------------------|\n",
      "| d_mult          | int    | 1         |     0   |    4    | transform_power_2_int |\n",
      "| l1              | int    | 3         |     3   |    8    | transform_power_2_int |\n",
      "| dim_feedforward | int    | 3         |     2   |    4    | transform_power_2_int |\n",
      "| nhead           | int    | 1         |     1   |    5    | transform_power_2_int |\n",
      "| num_layers      | int    | 6         |     2   |   12    | None                  |\n",
      "| epochs          | int    | 4         |     4   |    9    | transform_power_2_int |\n",
      "| batch_size      | int    | 4         |     5   |   10    | transform_power_2_int |\n",
      "| act_fn          | factor | Tanh      |     0   |    1    | None                  |\n",
      "| optimizer       | factor | Adam      |     0   |    9    | None                  |\n",
      "| dropout_prob    | float  | 0.01      |     0   |    0.25 | None                  |\n",
      "| lr_mult         | float  | 1.0       |     0.1 |   10    | None                  |\n",
      "| patience        | int    | 2         |     2   |    6    | transform_power_2_int |\n",
      "| initialization  | factor | Default   |     0   |    2    | None                  |\n",
      "\n",
      "In fun(): config:\n",
      "{'act_fn': ReLU(),\n",
      " 'batch_size': 128,\n",
      " 'd_mult': 4,\n",
      " 'dim_feedforward': 8,\n",
      " 'dropout_prob': 0.17125447112036726,\n",
      " 'epochs': 16,\n",
      " 'initialization': 'Kaiming',\n",
      " 'l1': 64,\n",
      " 'lr_mult': 2.5535987488712726,\n",
      " 'nhead': 8,\n",
      " 'num_layers': 12,\n",
      " 'optimizer': 'NAdam',\n",
      " 'patience': 16}\n",
      "l_nodes: 64 must be divisible by nhead: 8 and 2.\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Error in fun(). Call to train_model failed. err=IndexError('tuple index out of range'), type(err)=<class 'IndexError'>\n",
      "Setting df_eval to np.nan\n",
      "\n",
      "\n",
      "In fun(): config:\n",
      "{'act_fn': ReLU(),\n",
      " 'batch_size': 32,\n",
      " 'd_mult': 4,\n",
      " 'dim_feedforward': 8,\n",
      " 'dropout_prob': 0.020925459645925255,\n",
      " 'epochs': 256,\n",
      " 'initialization': 'Xavier',\n",
      " 'l1': 16,\n",
      " 'lr_mult': 5.738504342984723,\n",
      " 'nhead': 4,\n",
      " 'num_layers': 3,\n",
      " 'optimizer': 'SGD',\n",
      " 'patience': 64}\n",
      "l_nodes: 32 must be divisible by nhead: 4 and 2.\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Error in fun(). Call to train_model failed. err=IndexError('tuple index out of range'), type(err)=<class 'IndexError'>\n",
      "Setting df_eval to np.nan\n",
      "\n",
      "\n",
      "In fun(): config:\n",
      "{'act_fn': Tanh(),\n",
      " 'batch_size': 512,\n",
      " 'd_mult': 16,\n",
      " 'dim_feedforward': 4,\n",
      " 'dropout_prob': 0.07175561290421134,\n",
      " 'epochs': 64,\n",
      " 'initialization': 'Kaiming',\n",
      " 'l1': 32,\n",
      " 'lr_mult': 8.684878137247283,\n",
      " 'nhead': 32,\n",
      " 'num_layers': 5,\n",
      " 'optimizer': 'AdamW',\n",
      " 'patience': 32}\n",
      "l_nodes: 1024 must be divisible by nhead: 32 and 2.\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Error in fun(). Call to train_model failed. err=IndexError('tuple index out of range'), type(err)=<class 'IndexError'>\n",
      "Setting df_eval to np.nan\n",
      "\n",
      "\n",
      "In fun(): config:\n",
      "{'act_fn': Tanh(),\n",
      " 'batch_size': 1024,\n",
      " 'd_mult': 1,\n",
      " 'dim_feedforward': 8,\n",
      " 'dropout_prob': 0.14597075659186082,\n",
      " 'epochs': 64,\n",
      " 'initialization': 'Kaiming',\n",
      " 'l1': 16,\n",
      " 'lr_mult': 0.9772843181961492,\n",
      " 'nhead': 16,\n",
      " 'num_layers': 9,\n",
      " 'optimizer': 'Adagrad',\n",
      " 'patience': 8}\n",
      "l_nodes: 32 must be divisible by nhead: 16 and 2.\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Error in fun(). Call to train_model failed. err=IndexError('tuple index out of range'), type(err)=<class 'IndexError'>\n",
      "Setting df_eval to np.nan\n",
      "\n",
      "\n",
      "In fun(): config:\n",
      "{'act_fn': ReLU(),\n",
      " 'batch_size': 64,\n",
      " 'd_mult': 2,\n",
      " 'dim_feedforward': 8,\n",
      " 'dropout_prob': 0.0933829032915055,\n",
      " 'epochs': 256,\n",
      " 'initialization': 'Xavier',\n",
      " 'l1': 256,\n",
      " 'lr_mult': 7.415953007560914,\n",
      " 'nhead': 8,\n",
      " 'num_layers': 7,\n",
      " 'optimizer': 'Adam',\n",
      " 'patience': 16}\n",
      "l_nodes: 32 must be divisible by nhead: 8 and 2.\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "Error in fun(). Call to train_model failed. err=IndexError('tuple index out of range'), type(err)=<class 'IndexError'>\n",
      "Setting df_eval to np.nan\n",
      "\n",
      "\n",
      "In fun(): config:\n",
      "{'act_fn': Tanh(),\n",
      " 'batch_size': 256,\n",
      " 'd_mult': 8,\n",
      " 'dim_feedforward': 16,\n",
      " 'dropout_prob': 0.2470687219468448,\n",
      " 'epochs': 128,\n",
      " 'initialization': 'Default',\n",
      " 'l1': 128,\n",
      " 'lr_mult': 4.244166678514681,\n",
      " 'nhead': 2,\n",
      " 'num_layers': 5,\n",
      " 'optimizer': 'RMSprop',\n",
      " 'patience': 8}\n",
      "l_nodes: 32 must be divisible by nhead: 2 and 2.\n",
      "LightDataModule: setup(). stage: None\n",
      "LightDataModule setup(): full_train_size: 0.9\n",
      "LightDataModule setup(): val_size: 0.09\n",
      "LightDataModule setup(): train_size: 0.81\n",
      "LightDataModule setup(): test_size: 0.1\n",
      "LightDataModule: setup(). stage: fit\n",
      "LightDataModule: setup(). stage: test\n",
      "LightDataModule: setup(). stage: predict\n",
      "train_model(): Test set size: 45\n",
      "train_model(): Train set size: 359\n",
      "train_model(): Batch size: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightDataModule: setup(). stage: TrainerFn.FITTING\n",
      "LightDataModule setup(): full_train_size: 0.9\n",
      "LightDataModule setup(): val_size: 0.09\n",
      "LightDataModule setup(): train_size: 0.81\n",
      "LightDataModule setup(): test_size: 0.1\n",
      "LightDataModule: setup(). stage: fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type                    | Params | In sizes      | Out sizes    \n",
      "--------------------------------------------------------------------------------------\n",
      "0 | embed     | SkipLinear              | 960    | [256, 10]     | [256, 320]   \n",
      "1 | pos_enc   | PositionalEncoding      | 0      | [256, 10, 32] | [256, 10, 32]\n",
      "2 | enc_layer | TransformerEncoderLayer | 5.4 K  | ?             | ?            \n",
      "3 | trans_enc | TransformerEncoder      | 27.1 K | [256, 10, 32] | [256, 10, 32]\n",
      "4 | layers    | Sequential              | 65.2 K | [256, 320]    | [256, 1]     \n",
      "--------------------------------------------------------------------------------------\n",
      "98.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "98.7 K    Total params\n",
      "0.395     Total estimated model params size (MB)\n",
      "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightDataModule: val_dataloader(). Training set size: 39\n",
      "LightDataModule: val_dataloader(). batch_size: 256\n",
      "LightDataModule: val_dataloader(). num_workers: 0\n",
      "LightDataModule: train_dataloader(). Training set size: 359\n",
      "LightDataModule: train_dataloader(). batch_size: 256\n",
      "LightDataModule: train_dataloader(). num_workers: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:293: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "I0212 23:46:37.907556 11727368192 plugin.py:429] Monitor runs begin\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.15.1 at http://localhost:6006/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "from spotGUI.tuner.spotRun import load_and_run_spot_python_experiment\n",
    "\n",
    "load_and_run_spot_python_experiment(\"spot_0000-0000_experiment.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spotCondaEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
